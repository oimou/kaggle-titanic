{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "raw_train = pd.read_csv(filepath_or_buffer=\"~/.kaggle/competitions/titanic/train.csv\")\n",
    "raw_test = pd.read_csv(filepath_or_buffer=\"~/.kaggle/competitions/titanic/test.csv\")\n",
    "\n",
    "raw_train[\"is_test\"] = False\n",
    "raw_test[\"is_test\"] = True\n",
    "\n",
    "all_data = pd.concat((raw_train, raw_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Name</th>\n",
       "      <th>Parch</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>is_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>113803</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>373450</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age Cabin Embarked     Fare  \\\n",
       "0  22.0   NaN        S   7.2500   \n",
       "1  38.0   C85        C  71.2833   \n",
       "2  26.0   NaN        S   7.9250   \n",
       "3  35.0  C123        S  53.1000   \n",
       "4  35.0   NaN        S   8.0500   \n",
       "\n",
       "                                                Name  Parch  PassengerId  \\\n",
       "0                            Braund, Mr. Owen Harris      0            1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...      0            2   \n",
       "2                             Heikkinen, Miss. Laina      0            3   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)      0            4   \n",
       "4                           Allen, Mr. William Henry      0            5   \n",
       "\n",
       "   Pclass     Sex  SibSp  Survived            Ticket  is_test  \n",
       "0       3    male      1       0.0         A/5 21171    False  \n",
       "1       1  female      1       1.0          PC 17599    False  \n",
       "2       3  female      0       1.0  STON/O2. 3101282    False  \n",
       "3       1  female      1       1.0            113803    False  \n",
       "4       3    male      0       0.0            373450    False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer to: [Titanic with Keras | Kaggle](https://www.kaggle.com/cstahl12/titanic-with-keras)\n",
    "\n",
    "def get_title_last_name(name):\n",
    "    full_name = name.str.split(', ', n=0, expand=True)\n",
    "    # last_name = full_name[0] # last_name は使っていない模様\n",
    "    titles = full_name[1].str.split('.', n=0, expand=True)\n",
    "    titles = titles[0]\n",
    "    return(titles)\n",
    "\n",
    "def get_titles_from_names(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df['Title'] = get_title_last_name(df['Name'])\n",
    "    df = df.drop(['Name'], axis=1)\n",
    "    return(df)\n",
    "\n",
    "def get_cabin_letter(df):\n",
    "    df['Cabin'].fillna('Z', inplace=True) # Cabin が空欄の行は \"Z\" で埋める\n",
    "    df['Cabin_letter'] = df['Cabin'].str[0] # ?例えば \"C91\" ならば \"C\" でまとめてしまっても問題ないということ？なぜ？\n",
    "    return(df)\n",
    "\n",
    "def get_dummy_cats(df):\n",
    "    return(pd.get_dummies(df, columns=['Title', 'Pclass', 'Sex', 'Embarked',\n",
    "                                       'Cabin', 'Cabin_letter'])) # なんだこの便利メソッドは！\n",
    "\n",
    "def process_data(df):\n",
    "    # preprocess titles, cabin, embarked\n",
    "    df = get_titles_from_names(df)\n",
    "    df['Embarked'].fillna('S', inplace=True) # よくよくデータを見ると、Embarkedが空欄の行がある\n",
    "    df = get_cabin_letter(df)\n",
    "    \n",
    "    # drop remaining features\n",
    "    df = df.drop(['Ticket', 'Fare'], axis=1) # ?なぜdropする？\n",
    "    \n",
    "    # create dummies for categorial features\n",
    "    df = get_dummy_cats(df)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "proc_data = process_data(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Network to predict missing ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_age_train = proc_data.drop(['Survived', 'is_test'], axis=1).dropna(axis=0)\n",
    "X_train_age = for_age_train.drop('Age', axis=1)\n",
    "y_train_age = for_age_train['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model to predict missing age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "tmodel = Sequential()\n",
    "tmodel.add(Dense(input_dim=X_train_age.shape[1], units=128,\n",
    "                 kernel_initializer='normal', bias_initializer='zeros'))\n",
    "tmodel.add(Activation('relu'))\n",
    "\n",
    "for i in range(0, 8):\n",
    "    tmodel.add(Dense(units=64, kernel_initializer='normal',\n",
    "                     bias_initializer='zeros'))\n",
    "    tmodel.add(Activation('relu'))\n",
    "    tmodel.add(Dropout(.25))\n",
    "\n",
    "tmodel.add(Dense(units=1))\n",
    "tmodel.add(Activation('linear'))\n",
    "\n",
    "tmodel.compile(loss='mean_squared_error', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 589.4844\n",
      "Epoch 2/100\n",
      " - 0s - loss: 512.4903\n",
      "Epoch 3/100\n",
      " - 0s - loss: 494.7624\n",
      "Epoch 4/100\n",
      " - 0s - loss: 461.7037\n",
      "Epoch 5/100\n",
      " - 0s - loss: 446.7656\n",
      "Epoch 6/100\n",
      " - 0s - loss: 398.8651\n",
      "Epoch 7/100\n",
      " - 0s - loss: 379.2041\n",
      "Epoch 8/100\n",
      " - 0s - loss: 329.6203\n",
      "Epoch 9/100\n",
      " - 0s - loss: 298.8570\n",
      "Epoch 10/100\n",
      " - 0s - loss: 246.3977\n",
      "Epoch 11/100\n",
      " - 0s - loss: 255.2395\n",
      "Epoch 12/100\n",
      " - 0s - loss: 229.6321\n",
      "Epoch 13/100\n",
      " - 0s - loss: 241.7561\n",
      "Epoch 14/100\n",
      " - 0s - loss: 231.2556\n",
      "Epoch 15/100\n",
      " - 0s - loss: 254.5543\n",
      "Epoch 16/100\n",
      " - 0s - loss: 232.6130\n",
      "Epoch 17/100\n",
      " - 0s - loss: 233.5638\n",
      "Epoch 18/100\n",
      " - 0s - loss: 228.8490\n",
      "Epoch 19/100\n",
      " - 0s - loss: 226.2028\n",
      "Epoch 20/100\n",
      " - 0s - loss: 226.6595\n",
      "Epoch 21/100\n",
      " - 0s - loss: 221.1596\n",
      "Epoch 22/100\n",
      " - 0s - loss: 233.2341\n",
      "Epoch 23/100\n",
      " - 0s - loss: 237.2354\n",
      "Epoch 24/100\n",
      " - 0s - loss: 214.3832\n",
      "Epoch 25/100\n",
      " - 0s - loss: 214.4217\n",
      "Epoch 26/100\n",
      " - 0s - loss: 229.7090\n",
      "Epoch 27/100\n",
      " - 0s - loss: 211.9812\n",
      "Epoch 28/100\n",
      " - 0s - loss: 220.5673\n",
      "Epoch 29/100\n",
      " - 0s - loss: 209.1763\n",
      "Epoch 30/100\n",
      " - 0s - loss: 195.7361\n",
      "Epoch 31/100\n",
      " - 0s - loss: 199.4064\n",
      "Epoch 32/100\n",
      " - 0s - loss: 206.9832\n",
      "Epoch 33/100\n",
      " - 0s - loss: 193.3845\n",
      "Epoch 34/100\n",
      " - 0s - loss: 184.2110\n",
      "Epoch 35/100\n",
      " - 0s - loss: 191.0778\n",
      "Epoch 36/100\n",
      " - 0s - loss: 198.5456\n",
      "Epoch 37/100\n",
      " - 0s - loss: 190.6764\n",
      "Epoch 38/100\n",
      " - 0s - loss: 191.1842\n",
      "Epoch 39/100\n",
      " - 0s - loss: 190.1488\n",
      "Epoch 40/100\n",
      " - 0s - loss: 202.4235\n",
      "Epoch 41/100\n",
      " - 0s - loss: 187.8073\n",
      "Epoch 42/100\n",
      " - 0s - loss: 187.1760\n",
      "Epoch 43/100\n",
      " - 0s - loss: 183.7212\n",
      "Epoch 44/100\n",
      " - 0s - loss: 189.3532\n",
      "Epoch 45/100\n",
      " - 0s - loss: 180.2708\n",
      "Epoch 46/100\n",
      " - 0s - loss: 182.0339\n",
      "Epoch 47/100\n",
      " - 0s - loss: 179.3684\n",
      "Epoch 48/100\n",
      " - 0s - loss: 180.7983\n",
      "Epoch 49/100\n",
      " - 0s - loss: 176.5554\n",
      "Epoch 50/100\n",
      " - 0s - loss: 168.1252\n",
      "Epoch 51/100\n",
      " - 0s - loss: 179.3684\n",
      "Epoch 52/100\n",
      " - 0s - loss: 178.8460\n",
      "Epoch 53/100\n",
      " - 0s - loss: 170.7710\n",
      "Epoch 54/100\n",
      " - 0s - loss: 176.0333\n",
      "Epoch 55/100\n",
      " - 0s - loss: 179.3420\n",
      "Epoch 56/100\n",
      " - 0s - loss: 171.4620\n",
      "Epoch 57/100\n",
      " - 0s - loss: 178.5640\n",
      "Epoch 58/100\n",
      " - 0s - loss: 170.9833\n",
      "Epoch 59/100\n",
      " - 0s - loss: 173.4412\n",
      "Epoch 60/100\n",
      " - 0s - loss: 174.3509\n",
      "Epoch 61/100\n",
      " - 0s - loss: 170.7676\n",
      "Epoch 62/100\n",
      " - 0s - loss: 173.6857\n",
      "Epoch 63/100\n",
      " - 0s - loss: 162.5085\n",
      "Epoch 64/100\n",
      " - 0s - loss: 158.8027\n",
      "Epoch 65/100\n",
      " - 0s - loss: 157.4356\n",
      "Epoch 66/100\n",
      " - 0s - loss: 175.2073\n",
      "Epoch 67/100\n",
      " - 0s - loss: 159.9970\n",
      "Epoch 68/100\n",
      " - 0s - loss: 161.8580\n",
      "Epoch 69/100\n",
      " - 0s - loss: 169.5876\n",
      "Epoch 70/100\n",
      " - 0s - loss: 162.2769\n",
      "Epoch 71/100\n",
      " - 0s - loss: 164.6587\n",
      "Epoch 72/100\n",
      " - 0s - loss: 159.2778\n",
      "Epoch 73/100\n",
      " - 0s - loss: 158.5858\n",
      "Epoch 74/100\n",
      " - 0s - loss: 162.6561\n",
      "Epoch 75/100\n",
      " - 0s - loss: 165.4483\n",
      "Epoch 76/100\n",
      " - 0s - loss: 166.2703\n",
      "Epoch 77/100\n",
      " - 0s - loss: 161.3736\n",
      "Epoch 78/100\n",
      " - 0s - loss: 159.1157\n",
      "Epoch 79/100\n",
      " - 0s - loss: 158.7299\n",
      "Epoch 80/100\n",
      " - 0s - loss: 164.6092\n",
      "Epoch 81/100\n",
      " - 0s - loss: 165.6925\n",
      "Epoch 82/100\n",
      " - 0s - loss: 157.0633\n",
      "Epoch 83/100\n",
      " - 0s - loss: 157.9883\n",
      "Epoch 84/100\n",
      " - 0s - loss: 161.5868\n",
      "Epoch 85/100\n",
      " - 0s - loss: 154.7110\n",
      "Epoch 86/100\n",
      " - 0s - loss: 166.8014\n",
      "Epoch 87/100\n",
      " - 0s - loss: 146.8554\n",
      "Epoch 88/100\n",
      " - 0s - loss: 157.0482\n",
      "Epoch 89/100\n",
      " - 0s - loss: 163.2085\n",
      "Epoch 90/100\n",
      " - 0s - loss: 162.9404\n",
      "Epoch 91/100\n",
      " - 0s - loss: 153.1241\n",
      "Epoch 92/100\n",
      " - 0s - loss: 148.7272\n",
      "Epoch 93/100\n",
      " - 0s - loss: 166.0430\n",
      "Epoch 94/100\n",
      " - 0s - loss: 150.9429\n",
      "Epoch 95/100\n",
      " - 0s - loss: 150.4034\n",
      "Epoch 96/100\n",
      " - 0s - loss: 156.7168\n",
      "Epoch 97/100\n",
      " - 0s - loss: 157.5229\n",
      "Epoch 98/100\n",
      " - 0s - loss: 151.7138\n",
      "Epoch 99/100\n",
      " - 0s - loss: 149.9757\n",
      "Epoch 100/100\n",
      " - 0s - loss: 141.9362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10aa052b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel.fit(X_train_age.values, y_train_age.values, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_train = proc_data[proc_data[\"is_test\"] == False].copy()\n",
    "proc_eval = proc_data[proc_data[\"is_test\"] == False].copy()\n",
    "proc_test = proc_data[proc_data[\"is_test\"] == True].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred = proc_train.loc[proc_train['Age'].isnull()].drop(\n",
    "          ['Age', 'Survived', 'is_test'], axis=1)\n",
    "p = tmodel.predict(to_pred.values)\n",
    "\n",
    "proc_train.loc[proc_train[\"Age\"].isnull(), (\"Age\")] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred = proc_test.loc[proc_test['Age'].isnull()].drop(\n",
    "          ['Age', 'Survived', 'is_test'], axis=1)\n",
    "p = tmodel.predict(to_pred.values)\n",
    "\n",
    "proc_test.loc[proc_test[\"Age\"].isnull(), (\"Age\")] = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build network to predict \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = proc_train.drop([\"Survived\", \"is_test\"], axis=1)\n",
    "y = pd.get_dummies(proc_train[\"Survived\"]) # ?なぜこうする？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 801 samples, validate on 90 samples\n",
      "Epoch 1/20\n",
      "801/801 [==============================] - 2s 3ms/step - loss: 0.6882 - acc: 0.6055 - val_loss: 0.6769 - val_acc: 0.6222\n",
      "Epoch 2/20\n",
      "801/801 [==============================] - 0s 173us/step - loss: 0.6716 - acc: 0.6155 - val_loss: 0.6629 - val_acc: 0.6222\n",
      "Epoch 3/20\n",
      "801/801 [==============================] - 0s 157us/step - loss: 0.6693 - acc: 0.6155 - val_loss: 0.6633 - val_acc: 0.6222\n",
      "Epoch 4/20\n",
      "801/801 [==============================] - 0s 154us/step - loss: 0.6670 - acc: 0.6155 - val_loss: 0.6643 - val_acc: 0.6222\n",
      "Epoch 5/20\n",
      "801/801 [==============================] - 0s 224us/step - loss: 0.6679 - acc: 0.6155 - val_loss: 0.6656 - val_acc: 0.6222\n",
      "Epoch 6/20\n",
      "801/801 [==============================] - 0s 180us/step - loss: 0.6679 - acc: 0.6155 - val_loss: 0.6641 - val_acc: 0.6222\n",
      "Epoch 7/20\n",
      "801/801 [==============================] - 0s 156us/step - loss: 0.6681 - acc: 0.6155 - val_loss: 0.6633 - val_acc: 0.6222\n",
      "Epoch 8/20\n",
      "801/801 [==============================] - 0s 177us/step - loss: 0.6659 - acc: 0.6155 - val_loss: 0.6636 - val_acc: 0.6222\n",
      "Epoch 9/20\n",
      "801/801 [==============================] - 0s 164us/step - loss: 0.6658 - acc: 0.6155 - val_loss: 0.6635 - val_acc: 0.6222\n",
      "Epoch 10/20\n",
      "801/801 [==============================] - 0s 168us/step - loss: 0.6694 - acc: 0.6155 - val_loss: 0.6633 - val_acc: 0.6222\n",
      "Epoch 11/20\n",
      "801/801 [==============================] - 0s 147us/step - loss: 0.6667 - acc: 0.6155 - val_loss: 0.6632 - val_acc: 0.6222\n",
      "Epoch 12/20\n",
      "801/801 [==============================] - 0s 169us/step - loss: 0.6678 - acc: 0.6155 - val_loss: 0.6630 - val_acc: 0.6222\n",
      "Epoch 13/20\n",
      "801/801 [==============================] - 0s 173us/step - loss: 0.6679 - acc: 0.6155 - val_loss: 0.6637 - val_acc: 0.6222\n",
      "Epoch 14/20\n",
      "801/801 [==============================] - 0s 163us/step - loss: 0.6674 - acc: 0.6155 - val_loss: 0.6648 - val_acc: 0.6222\n",
      "Epoch 15/20\n",
      "801/801 [==============================] - 0s 161us/step - loss: 0.6696 - acc: 0.6155 - val_loss: 0.6640 - val_acc: 0.6222\n",
      "Epoch 16/20\n",
      "801/801 [==============================] - 0s 188us/step - loss: 0.6697 - acc: 0.6155 - val_loss: 0.6631 - val_acc: 0.6222\n",
      "Epoch 17/20\n",
      "801/801 [==============================] - 0s 175us/step - loss: 0.6705 - acc: 0.6155 - val_loss: 0.6632 - val_acc: 0.6222\n",
      "Epoch 18/20\n",
      "801/801 [==============================] - 0s 149us/step - loss: 0.6675 - acc: 0.6155 - val_loss: 0.6640 - val_acc: 0.6222\n",
      "Epoch 19/20\n",
      "801/801 [==============================] - 0s 205us/step - loss: 0.6674 - acc: 0.6155 - val_loss: 0.6643 - val_acc: 0.6222\n",
      "Epoch 20/20\n",
      "801/801 [==============================] - 0s 169us/step - loss: 0.6672 - acc: 0.6155 - val_loss: 0.6631 - val_acc: 0.6222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c7ca898>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=X.shape[1], units=128,\n",
    "                 kernel_initializer='normal', bias_initializer='zeros'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "for i in range(0, 15):\n",
    "    model.add(Dense(units=128, kernel_initializer='normal',\n",
    "                     bias_initializer='zeros'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.40))\n",
    "\n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X.values, y.values,\n",
    "          validation_split=0.1,\n",
    "          epochs=20,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6126464 , 0.38735363],\n",
       "       [0.61273855, 0.38726145],\n",
       "       [0.6131553 , 0.3868447 ],\n",
       "       [0.6125196 , 0.38748035],\n",
       "       [0.61226666, 0.38773334],\n",
       "       [0.61229616, 0.3877038 ],\n",
       "       [0.612394  , 0.38760602],\n",
       "       [0.6124441 , 0.3875558 ],\n",
       "       [0.612193  , 0.387807  ],\n",
       "       [0.61242247, 0.38757753],\n",
       "       [0.61260325, 0.3873967 ],\n",
       "       [0.6128574 , 0.38714266],\n",
       "       [0.6122015 , 0.38779846],\n",
       "       [0.6132221 , 0.3867779 ],\n",
       "       [0.6126235 , 0.38737646],\n",
       "       [0.6122621 , 0.38773796],\n",
       "       [0.6126379 , 0.3873621 ],\n",
       "       [0.61240524, 0.38759482],\n",
       "       [0.6123919 , 0.38760817],\n",
       "       [0.6126939 , 0.3873061 ],\n",
       "       [0.6130184 , 0.3869816 ],\n",
       "       [0.6122008 , 0.38779923],\n",
       "       [0.6124816 , 0.38751838],\n",
       "       [0.6123579 , 0.38764206],\n",
       "       [0.612573  , 0.38742697],\n",
       "       [0.6130417 , 0.38695833],\n",
       "       [0.61217713, 0.38782284],\n",
       "       [0.61245483, 0.38754514],\n",
       "       [0.612724  , 0.387276  ],\n",
       "       [0.61255926, 0.3874407 ],\n",
       "       [0.6129984 , 0.3870016 ],\n",
       "       [0.6124835 , 0.3875165 ],\n",
       "       [0.61250716, 0.3874928 ],\n",
       "       [0.61246306, 0.38753688],\n",
       "       [0.61247563, 0.38752437],\n",
       "       [0.6124124 , 0.38758767],\n",
       "       [0.6124117 , 0.38758835],\n",
       "       [0.6123441 , 0.38765594],\n",
       "       [0.61256486, 0.38743508],\n",
       "       [0.6126723 , 0.38732764],\n",
       "       [0.6127932 , 0.38720676],\n",
       "       [0.6126365 , 0.3873635 ],\n",
       "       [0.612899  , 0.38710102],\n",
       "       [0.61246145, 0.38753855],\n",
       "       [0.61264503, 0.38735503],\n",
       "       [0.61258185, 0.38741815],\n",
       "       [0.6128037 , 0.3871963 ],\n",
       "       [0.6126764 , 0.38732365],\n",
       "       [0.61291665, 0.38708332],\n",
       "       [0.61260086, 0.3873991 ],\n",
       "       [0.6124577 , 0.3875423 ],\n",
       "       [0.6125461 , 0.3874539 ],\n",
       "       [0.61232054, 0.3876795 ],\n",
       "       [0.6123668 , 0.38763326],\n",
       "       [0.6126138 , 0.38738623],\n",
       "       [0.6122961 , 0.38770387],\n",
       "       [0.6128092 , 0.38719076],\n",
       "       [0.6125435 , 0.38745645],\n",
       "       [0.6127152 , 0.38728482],\n",
       "       [0.61248744, 0.3875126 ],\n",
       "       [0.6124874 , 0.38751262],\n",
       "       [0.61270267, 0.3872973 ],\n",
       "       [0.6125083 , 0.3874917 ],\n",
       "       [0.61241186, 0.3875881 ],\n",
       "       [0.61222374, 0.38777623],\n",
       "       [0.6125458 , 0.38745424],\n",
       "       [0.612359  , 0.38764107],\n",
       "       [0.6130121 , 0.38698792],\n",
       "       [0.6125737 , 0.38742632],\n",
       "       [0.6129612 , 0.38703877],\n",
       "       [0.61246127, 0.38753876],\n",
       "       [0.6125782 , 0.3874218 ],\n",
       "       [0.6125609 , 0.38743904],\n",
       "       [0.6125486 , 0.3874514 ],\n",
       "       [0.61251324, 0.3874868 ],\n",
       "       [0.61262053, 0.3873795 ],\n",
       "       [0.6127605 , 0.3872395 ],\n",
       "       [0.61292976, 0.38707027],\n",
       "       [0.61270356, 0.38729644],\n",
       "       [0.6124846 , 0.38751546],\n",
       "       [0.61228013, 0.3877198 ],\n",
       "       [0.6133703 , 0.3866297 ],\n",
       "       [0.6130885 , 0.3869115 ],\n",
       "       [0.6127774 , 0.3872226 ],\n",
       "       [0.61272436, 0.3872756 ],\n",
       "       [0.6127272 , 0.38727275],\n",
       "       [0.61254954, 0.38745046],\n",
       "       [0.6124291 , 0.38757086],\n",
       "       [0.6126264 , 0.38737372],\n",
       "       [0.6122374 , 0.3877626 ],\n",
       "       [0.61250263, 0.38749743],\n",
       "       [0.6127968 , 0.38720322],\n",
       "       [0.6124553 , 0.38754472],\n",
       "       [0.6128017 , 0.38719836],\n",
       "       [0.6126153 , 0.3873847 ],\n",
       "       [0.61270416, 0.38729584],\n",
       "       [0.613421  , 0.38657898],\n",
       "       [0.612785  , 0.387215  ],\n",
       "       [0.6124879 , 0.3875121 ],\n",
       "       [0.61287147, 0.3871285 ],\n",
       "       [0.6127088 , 0.3872912 ],\n",
       "       [0.6127082 , 0.38729173],\n",
       "       [0.6128083 , 0.3871917 ],\n",
       "       [0.6127421 , 0.3872579 ],\n",
       "       [0.612407  , 0.38759303],\n",
       "       [0.61278516, 0.3872148 ],\n",
       "       [0.6126511 , 0.38734886],\n",
       "       [0.6128204 , 0.38717958],\n",
       "       [0.61283803, 0.38716194],\n",
       "       [0.6125329 , 0.38746712],\n",
       "       [0.6129602 , 0.38703984],\n",
       "       [0.6126104 , 0.38738963],\n",
       "       [0.6126321 , 0.38736793],\n",
       "       [0.61248964, 0.38751036],\n",
       "       [0.6131833 , 0.38681665],\n",
       "       [0.61261183, 0.3873881 ],\n",
       "       [0.6128119 , 0.38718805],\n",
       "       [0.6122078 , 0.38779223],\n",
       "       [0.6127905 , 0.38720953],\n",
       "       [0.6126432 , 0.38735688],\n",
       "       [0.6123885 , 0.38761157],\n",
       "       [0.6128535 , 0.3871465 ],\n",
       "       [0.61263585, 0.3873642 ],\n",
       "       [0.61282957, 0.3871704 ],\n",
       "       [0.6128618 , 0.3871382 ],\n",
       "       [0.6125032 , 0.38749686],\n",
       "       [0.61273384, 0.3872662 ],\n",
       "       [0.61259884, 0.38740116],\n",
       "       [0.6130683 , 0.38693178],\n",
       "       [0.61277294, 0.387227  ],\n",
       "       [0.61292624, 0.38707376],\n",
       "       [0.6131068 , 0.38689324],\n",
       "       [0.61269164, 0.3873084 ],\n",
       "       [0.6128462 , 0.38715377],\n",
       "       [0.6131593 , 0.38684067],\n",
       "       [0.6127885 , 0.38721153],\n",
       "       [0.6127964 , 0.38720354],\n",
       "       [0.61278206, 0.38721788],\n",
       "       [0.6126359 , 0.38736403],\n",
       "       [0.61302704, 0.38697296],\n",
       "       [0.61244285, 0.38755715],\n",
       "       [0.6127481 , 0.3872519 ],\n",
       "       [0.613301  , 0.38669908],\n",
       "       [0.61282986, 0.38717005],\n",
       "       [0.61309415, 0.38690588],\n",
       "       [0.6129478 , 0.3870522 ],\n",
       "       [0.61289877, 0.3871013 ],\n",
       "       [0.6127878 , 0.38721222],\n",
       "       [0.61291933, 0.3870807 ],\n",
       "       [0.61283493, 0.38716507],\n",
       "       [0.6125202 , 0.38747975],\n",
       "       [0.6128995 , 0.38710052],\n",
       "       [0.6135516 , 0.38644835],\n",
       "       [0.6128608 , 0.38713923],\n",
       "       [0.61260414, 0.38739583],\n",
       "       [0.6128401 , 0.38715982],\n",
       "       [0.61266696, 0.38733304],\n",
       "       [0.6126845 , 0.38731554],\n",
       "       [0.6130409 , 0.38695905],\n",
       "       [0.6127164 , 0.38728362],\n",
       "       [0.6127363 , 0.38726372],\n",
       "       [0.6125083 , 0.3874917 ],\n",
       "       [0.6127013 , 0.3872987 ],\n",
       "       [0.6129732 , 0.38702685],\n",
       "       [0.6130928 , 0.38690725],\n",
       "       [0.6127447 , 0.38725528],\n",
       "       [0.6131391 , 0.3868609 ],\n",
       "       [0.6127569 , 0.38724306],\n",
       "       [0.61280096, 0.38719898],\n",
       "       [0.6127001 , 0.38729987],\n",
       "       [0.6129903 , 0.3870097 ],\n",
       "       [0.61289525, 0.38710472],\n",
       "       [0.6128701 , 0.38712987],\n",
       "       [0.6129553 , 0.38704473],\n",
       "       [0.6131247 , 0.38687533],\n",
       "       [0.612555  , 0.387445  ],\n",
       "       [0.61265045, 0.38734955],\n",
       "       [0.6132933 , 0.38670668],\n",
       "       [0.61279833, 0.38720167],\n",
       "       [0.61328536, 0.38671464],\n",
       "       [0.6129591 , 0.3870409 ],\n",
       "       [0.612948  , 0.38705194],\n",
       "       [0.6125787 , 0.38742128],\n",
       "       [0.6130077 , 0.38699228],\n",
       "       [0.61266994, 0.38733003],\n",
       "       [0.61316335, 0.3868367 ],\n",
       "       [0.61268026, 0.38731974],\n",
       "       [0.6128157 , 0.3871843 ],\n",
       "       [0.61254144, 0.3874586 ],\n",
       "       [0.6131728 , 0.38682723],\n",
       "       [0.61305726, 0.38694277],\n",
       "       [0.6130237 , 0.38697633],\n",
       "       [0.61268413, 0.38731584],\n",
       "       [0.61358684, 0.38641313],\n",
       "       [0.6125886 , 0.38741142],\n",
       "       [0.6131025 , 0.38689753],\n",
       "       [0.612468  , 0.38753197],\n",
       "       [0.61271334, 0.38728666],\n",
       "       [0.6128927 , 0.38710728],\n",
       "       [0.6129148 , 0.38708517],\n",
       "       [0.6128403 , 0.38715965],\n",
       "       [0.6125335 , 0.38746646],\n",
       "       [0.61315554, 0.38684443],\n",
       "       [0.6125384 , 0.38746163],\n",
       "       [0.6129403 , 0.38705966],\n",
       "       [0.6130144 , 0.38698563],\n",
       "       [0.6129853 , 0.38701466],\n",
       "       [0.6129323 , 0.3870677 ],\n",
       "       [0.6128358 , 0.38716424],\n",
       "       [0.61299616, 0.38700384],\n",
       "       [0.6131198 , 0.38688025],\n",
       "       [0.61309063, 0.3869093 ],\n",
       "       [0.61283386, 0.38716623],\n",
       "       [0.61345124, 0.3865488 ],\n",
       "       [0.61305624, 0.38694373],\n",
       "       [0.61317766, 0.38682234],\n",
       "       [0.6128824 , 0.38711762],\n",
       "       [0.61355764, 0.3864424 ],\n",
       "       [0.61311495, 0.3868851 ],\n",
       "       [0.6131103 , 0.38688967],\n",
       "       [0.6128813 , 0.38711867],\n",
       "       [0.61296403, 0.38703594],\n",
       "       [0.612734  , 0.38726604],\n",
       "       [0.6129692 , 0.3870308 ],\n",
       "       [0.613278  , 0.38672203],\n",
       "       [0.61292154, 0.38707843],\n",
       "       [0.6130084 , 0.38699156],\n",
       "       [0.61291164, 0.38708833],\n",
       "       [0.6133318 , 0.38666824],\n",
       "       [0.6131879 , 0.3868121 ],\n",
       "       [0.61283463, 0.3871653 ],\n",
       "       [0.61279947, 0.38720056],\n",
       "       [0.61299425, 0.3870058 ],\n",
       "       [0.61312973, 0.38687027],\n",
       "       [0.61313343, 0.38686666],\n",
       "       [0.6129844 , 0.38701564],\n",
       "       [0.6136219 , 0.38637814],\n",
       "       [0.61295563, 0.3870444 ],\n",
       "       [0.6127724 , 0.38722762],\n",
       "       [0.61315024, 0.38684973],\n",
       "       [0.61335605, 0.38664395],\n",
       "       [0.6131922 , 0.3868078 ],\n",
       "       [0.6132461 , 0.38675395],\n",
       "       [0.6131691 , 0.38683087],\n",
       "       [0.612854  , 0.387146  ],\n",
       "       [0.6132269 , 0.38677308],\n",
       "       [0.61286306, 0.38713694],\n",
       "       [0.61333853, 0.38666144],\n",
       "       [0.6129757 , 0.38702428],\n",
       "       [0.6130081 , 0.38699192],\n",
       "       [0.6125546 , 0.38744536],\n",
       "       [0.6130256 , 0.3869744 ],\n",
       "       [0.61299336, 0.3870066 ],\n",
       "       [0.61309314, 0.38690677],\n",
       "       [0.6132359 , 0.38676408],\n",
       "       [0.61319846, 0.3868015 ],\n",
       "       [0.61318684, 0.38681316],\n",
       "       [0.6131657 , 0.38683435],\n",
       "       [0.6128477 , 0.38715228],\n",
       "       [0.6130617 , 0.38693827],\n",
       "       [0.61332875, 0.3866712 ],\n",
       "       [0.61306685, 0.38693312],\n",
       "       [0.6129877 , 0.38701233],\n",
       "       [0.6126325 , 0.38736752],\n",
       "       [0.6131385 , 0.38686153],\n",
       "       [0.61322314, 0.38677686],\n",
       "       [0.61320853, 0.38679153],\n",
       "       [0.6132281 , 0.3867719 ],\n",
       "       [0.6130135 , 0.38698652],\n",
       "       [0.6130248 , 0.38697523],\n",
       "       [0.6133529 , 0.3866471 ],\n",
       "       [0.6132244 , 0.3867756 ],\n",
       "       [0.6128959 , 0.38710412],\n",
       "       [0.61300755, 0.38699245],\n",
       "       [0.6132098 , 0.38679022],\n",
       "       [0.61290926, 0.3870908 ],\n",
       "       [0.6131714 , 0.38682857],\n",
       "       [0.6133835 , 0.38661647],\n",
       "       [0.6132095 , 0.38679054],\n",
       "       [0.6130859 , 0.38691404],\n",
       "       [0.6130033 , 0.38699666],\n",
       "       [0.61275727, 0.38724276],\n",
       "       [0.6130555 , 0.38694447],\n",
       "       [0.61276525, 0.38723478],\n",
       "       [0.61269975, 0.38730028],\n",
       "       [0.6133771 , 0.38662285],\n",
       "       [0.6132759 , 0.38672414],\n",
       "       [0.61306405, 0.38693598],\n",
       "       [0.6131737 , 0.38682625],\n",
       "       [0.6132835 , 0.38671646],\n",
       "       [0.6132688 , 0.3867312 ],\n",
       "       [0.61312306, 0.38687694],\n",
       "       [0.6132556 , 0.38674438],\n",
       "       [0.6135316 , 0.38646844],\n",
       "       [0.613399  , 0.38660094],\n",
       "       [0.613231  , 0.38676897],\n",
       "       [0.61264884, 0.38735116],\n",
       "       [0.61324316, 0.3867568 ],\n",
       "       [0.61325073, 0.38674924],\n",
       "       [0.61328816, 0.38671187],\n",
       "       [0.61333936, 0.38666067],\n",
       "       [0.6131852 , 0.3868148 ],\n",
       "       [0.61348754, 0.38651243],\n",
       "       [0.6132202 , 0.38677976],\n",
       "       [0.6131126 , 0.3868873 ],\n",
       "       [0.61364365, 0.38635635],\n",
       "       [0.61318934, 0.38681066],\n",
       "       [0.6128241 , 0.38717595],\n",
       "       [0.613651  , 0.38634893],\n",
       "       [0.6134352 , 0.3865648 ],\n",
       "       [0.613145  , 0.386855  ],\n",
       "       [0.6131761 , 0.38682386],\n",
       "       [0.61334217, 0.3866578 ],\n",
       "       [0.6132878 , 0.38671225],\n",
       "       [0.6134523 , 0.38654774],\n",
       "       [0.6129916 , 0.38700837],\n",
       "       [0.6136729 , 0.38632712],\n",
       "       [0.6131365 , 0.38686356],\n",
       "       [0.6133054 , 0.3866947 ],\n",
       "       [0.61319125, 0.38680875],\n",
       "       [0.6132948 , 0.38670522],\n",
       "       [0.613187  , 0.38681298],\n",
       "       [0.61319333, 0.38680667],\n",
       "       [0.6133611 , 0.3866389 ],\n",
       "       [0.6133119 , 0.38668814],\n",
       "       [0.6132607 , 0.38673928],\n",
       "       [0.61285573, 0.38714427],\n",
       "       [0.6135648 , 0.3864352 ],\n",
       "       [0.6133213 , 0.38667873],\n",
       "       [0.61319757, 0.38680246],\n",
       "       [0.61346555, 0.38653436],\n",
       "       [0.61338395, 0.38661605],\n",
       "       [0.6133578 , 0.38664222],\n",
       "       [0.61304533, 0.38695472],\n",
       "       [0.6133461 , 0.38665387],\n",
       "       [0.6132763 , 0.38672373],\n",
       "       [0.613387  , 0.386613  ],\n",
       "       [0.61349297, 0.386507  ],\n",
       "       [0.61328256, 0.3867174 ],\n",
       "       [0.6131801 , 0.3868199 ],\n",
       "       [0.6131801 , 0.38681984],\n",
       "       [0.6134423 , 0.38655773],\n",
       "       [0.6131645 , 0.38683552],\n",
       "       [0.6135674 , 0.38643256],\n",
       "       [0.61318094, 0.38681912],\n",
       "       [0.61306953, 0.38693044],\n",
       "       [0.61331856, 0.38668138],\n",
       "       [0.6133758 , 0.38662428],\n",
       "       [0.6132925 , 0.38670745],\n",
       "       [0.6132598 , 0.38674018],\n",
       "       [0.61336386, 0.38663614],\n",
       "       [0.61331576, 0.38668424],\n",
       "       [0.61321074, 0.3867892 ],\n",
       "       [0.61371756, 0.38628247],\n",
       "       [0.6128479 , 0.38715205],\n",
       "       [0.6136773 , 0.3863227 ],\n",
       "       [0.6136749 , 0.38632506],\n",
       "       [0.61345726, 0.3865427 ],\n",
       "       [0.61344594, 0.3865541 ],\n",
       "       [0.613317  , 0.38668293],\n",
       "       [0.613169  , 0.38683096],\n",
       "       [0.61315185, 0.3868481 ],\n",
       "       [0.61329603, 0.386704  ],\n",
       "       [0.61342055, 0.38657948],\n",
       "       [0.61312026, 0.38687977],\n",
       "       [0.61316705, 0.386833  ],\n",
       "       [0.6134388 , 0.38656121],\n",
       "       [0.61321276, 0.38678724],\n",
       "       [0.6134754 , 0.38652462],\n",
       "       [0.61339104, 0.38660902],\n",
       "       [0.6133039 , 0.38669613],\n",
       "       [0.61322004, 0.38677993],\n",
       "       [0.6136941 , 0.38630596],\n",
       "       [0.6136906 , 0.38630942],\n",
       "       [0.61361974, 0.3863803 ],\n",
       "       [0.6135023 , 0.38649768],\n",
       "       [0.61323935, 0.3867606 ],\n",
       "       [0.61332   , 0.38668007],\n",
       "       [0.6138328 , 0.3861672 ],\n",
       "       [0.6130724 , 0.38692757],\n",
       "       [0.613502  , 0.38649803],\n",
       "       [0.61343473, 0.38656524],\n",
       "       [0.61338323, 0.3866168 ],\n",
       "       [0.61321485, 0.38678515],\n",
       "       [0.6134871 , 0.3865129 ],\n",
       "       [0.6132311 , 0.3867689 ],\n",
       "       [0.6134325 , 0.3865674 ],\n",
       "       [0.61398625, 0.38601378],\n",
       "       [0.61337477, 0.38662517],\n",
       "       [0.6131189 , 0.38688114],\n",
       "       [0.6133113 , 0.3866887 ],\n",
       "       [0.61359525, 0.3864047 ],\n",
       "       [0.6132121 , 0.3867879 ],\n",
       "       [0.6137986 , 0.38620132],\n",
       "       [0.6135249 , 0.38647512],\n",
       "       [0.61312807, 0.38687193],\n",
       "       [0.61344194, 0.3865581 ],\n",
       "       [0.61353326, 0.3864667 ],\n",
       "       [0.6134321 , 0.3865679 ],\n",
       "       [0.61355835, 0.3864416 ],\n",
       "       [0.61330754, 0.38669252],\n",
       "       [0.6136498 , 0.3863502 ],\n",
       "       [0.6132165 , 0.38678342],\n",
       "       [0.6133188 , 0.38668117],\n",
       "       [0.6136212 , 0.38637882],\n",
       "       [0.61328673, 0.38671324],\n",
       "       [0.6134276 , 0.38657242],\n",
       "       [0.6137431 , 0.3862568 ],\n",
       "       [0.61338335, 0.38661665],\n",
       "       [0.61303836, 0.38696164],\n",
       "       [0.6133886 , 0.38661137],\n",
       "       [0.61343044, 0.38656956],\n",
       "       [0.6134178 , 0.3865822 ],\n",
       "       [0.6136001 , 0.38639984],\n",
       "       [0.613474  , 0.38652596],\n",
       "       [0.61374044, 0.38625953],\n",
       "       [0.61360776, 0.3863922 ],\n",
       "       [0.6133111 , 0.38668895]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = proc_test.drop([\"Survived\", \"is_test\"], axis=1)\n",
    "\n",
    "p_survived = model.predict(X_test.values, batch_size=128)\n",
    "\n",
    "p_survived"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
